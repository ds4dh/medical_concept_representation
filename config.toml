[run]
  # Possible models: 'fasttext', 'glove', 'elmo',
  #                  'bert', 'bert_classifier', 'transformer'
  exp_id = 'test'
  model_used = 'fasttext'
  model_version = 0  # bert_classifier uses pretrained bert from same version
  ngram_min_len = 0  # 0 for word encoding, > 0 for subword encoding
  ngram_max_len = 0  # 0 for word encoding, > 0 for subword encoding
  debug = false
  load_model = false  # note: changing ngram_len will change embedding layer
  load_tokenizer = true  # TODO: implement this
  num_workers = 0  # TODO: check for potential issues with iter-style dataset for num_workers > 0
  pin_memory = false

[data]
  data_dir = './data/datasets/autophe'
  data_subdir = 'time_not_categorized'
  # data_dir = './data/datasets/chempred/uspto-mit/reagent_pred'
  # data_subdir = 'smiles'
  log_dir = './logs'
  max_seq_len = 512

[train]
  optimizer = 'adamw'  # adam, radam, adamw
  scheduler = 'linear'  # 'noam', 'linear'
  lr = 0.001  # initial learning rate (after warmup) (note: for noam scheduler, actual value is lower)
  betas = [0.9, 0.998]  # momentum
  weight_decay = 0.01  # weight decay (note: [adam, radam] and adamw have different ways of applying weight decay)
  accumulate_grad_batches = 1
  max_tokens_per_batch = 4096
  n_steps = 500000
  n_warmup_steps = 0

[models]
  [models.fasttext]
    task = 'skipgram'
    input_keys = ['center']
    label_keys = ['context']
    d_embed = 256
    [models.fasttext.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1

  [models.glove]
    task = 'cooc'
    input_keys = ['left', 'right']
    label_keys = ['cooc']
    d_embed = 256
    [models.glove.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
  
  [models.elmo]
    task = 'lm'
    input_keys = ['sequence']
    label_keys = ['sequence']
    d_convs = [512, 512, 512, 512]
    k_sizes = [2, 3, 4, 5]  # should be ngram_len lists?
    d_embed_char = 256
    d_embed_word = 256
    n_lstm_layers = 2
    dropout = 0.5
    [models.elmo.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
  
  [models.transformer]
    task = 'reagent_pred_mt'  # could add 'mt' here
    input_keys = ['src', 'tgt']  # 'tgt' present for teacher forcing
    label_keys = ['tgt']
    share_embeddings = true
    n_enc_layers = 4
    n_dec_layers = 4
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.transformer.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4

  [models.bert]
    task = 'reagent_pred_mlm'  # 'mlm', 'reagent_pred_mlm'
    input_keys = ['masked']
    label_keys = ['masked', 'target']  # 'masked' present to get mask indices
    n_layers = 4
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.bert.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
      '[MASK]' = 5

  [models.bert_classifier]
    # - If load_pretrained_bert is true, bert_ckpt_path is used for bert params.
    # - If bert_ckpt_path is 'none', bert_classifier parameters will be used to
    #   identify the correct checkpoint for bert (may fail).
    # - Note: for fine-tuning, you might consider decreasing the learning rate.
    task = 'reagent_pred_cls'  # could add 'cls' here
    tokenizer_task = 'reagent_pred_mlm'   # to have the same vocabulary as bert
    input_keys = ['sample']
    label_keys = ['label']
    n_classes = 100  # 0: all reagents, > 0: most popular reagents
    load_pretrained_bert = true
    bert_path = './logs/bert_ngram-min0-max0_reagent_prediction/version_0/'
    bert_grad_type = 'all'  # 'none', 'norm', 'all' (weights tuned in bert)
