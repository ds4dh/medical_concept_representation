[run]
  # Possible models: 'fasttext', 'glove', 'elmo', 'bert', 'bert_classifier'
  model_used = 'bert_classifier'
  model_id = 'reagent_prediction'
  model_version = 0  # bert_classifier uses pretrained bert from same version
  ngram_min_len = 0  # 0 for word encoding, > 0 for subword encoding
  ngram_max_len = 0  # 0 for word encoding, > 0 for subword encoding
  debug = true
  load_model = false  # note: changing ngram_len will change embedding layer
  load_tokenizer = true
  num_workers = 0  # TODO: check for potential issues with iter-style dataset for num_workers > 0
  pin_memory = false

[data]
  data_dir = '/mnt/d/DL/datasets/autophe/json'  #'./data/datasets/autophe'  # /autophe, /reagent_pred
  data_subdir = 'time_not_categorized'  # time_not_categorized, smiles
  log_dir = './logs'
  max_seq_len = 512

[train]
  lr = 0.2 #  2.0
  adam_betas = [0.9, 0.998]  # momentum
  adam_lambda = 0.01  # weight decay
  accumulate_grad_batches = 4
  max_tokens_per_batch = 4096
  n_steps = 500000
  n_warmup_steps = 10000

[models]
  [models.fasttext]
    task = 'skipgram'
    input_keys = ['center']
    label_keys = ['context']
    d_embed = 256
    [models.fasttext.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1

  [models.glove]
    task = 'cooc'
    input_keys = ['left', 'right']
    label_keys = ['cooc']
    d_embed = 256
    [models.glove.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      
  [models.bert]
    task = 'mlm'
    input_keys = ['masked']
    label_keys = ['masked', 'target']
    n_layers = 4  # 6
    d_embed = 256  # 384
    d_ff = 2048  # 3072
    n_heads = 8  # 12
    dropout = 0.1
    [models.bert.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
      '[MASK]' = 5

  # I added this for my chemistry project, but this might as well be used to
  # train a classifier on top of any model's pre-trained embeddings
  [models.bert_classifier]  # uses [model.bert] hyper-parameters
    task = 'reagent_pred'
    input_keys = ['sample']
    label_keys = ['reagent_label']
    n_classes = 0  # 0: all reagents, > 0: most popular reagents
    grad_only_for_norm_layers = true
