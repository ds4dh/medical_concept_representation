[run]
  model_used = 'glove'  # 'fasttext', 'glove', 'elmo', 'bert'
  model_id = 'test'
  encoding = 'word'  # 'word' / 'subword'
  debug_mode = true
  load_model = false
  load_tokenizer = true
  find_best_lr = false
  num_workers = 0  # TODO: check for potential issues with iter-style dataset for num_workers > 0
  pin_memory = false

[data]
  data_dir = './data/json'  # '/mnt/d/DL/datasets/autophe/json' - './data/json'
  data_subdir = 'time_not_categorized'
  log_dir = './logs'
  data_keys = []  # [] - ['text'] - ['src', 'tgt']
  max_len = 512
  # [data.special_tokens]
  #   ':OK' = 101
  #   ':AB' = 102

[train]
  lr = 2.0
  adam_betas = [0.9, 0.998]
  accumulate_grad_batches = 1
  max_tokens_per_batch = 4096
  n_steps = 500000
  n_warmup_steps = 8000

[models]
  [models.fasttext]
    task = 'skipgram'  # 'cbow'?
    d_embed = 256
    [models.fasttext.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1

  [models.glove]
    task = 'cooc'
    input_keys = ['left', 'right']
    label_keys = ['cooc']
    d_embed = 256
    [models.glove.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      
  [models.bert]
    task = 'mlm'
    n_layers = 4
    max_len = 512
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.bert.special_tokens]
      '[PAD]' = 0
      '[CLS]' = 1
      '[SEP]' = 2
      '[END]' = 3
      '[UNK]' = 4
      '[MASK]' = 5
