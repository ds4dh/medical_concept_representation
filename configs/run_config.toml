[run]
  exp_id = 'test'
  model_used = 'glove'
  model_version = 0  # will increase to avoid overwriting until version is unique
  token_shuffle_prob = 1.0  # shuffling may be applied inside sequence samples, with a certain probability
  ngram_mode = 'word'
  ngram_min_len = 2  # only used for ngram_mode == 'subword'; must be > 0
  ngram_max_len = 5  # only used for ngram_mode == 'subword'; must be >= ngram_min_len
  ngram_base_prefixes = ['DEM_', 'LOC_', 'LAB_', 'MED_', 'PRO_', 'DIA_']  # only used for ngram_mode != 'word'
  ngram_base_suffixes = [':OK', ':AB']  # only used for ngram_mode != 'word'
  debug = false  # if true, will use the smaller validation data as the training data
  load_model = false  # note: changing ngram_len will change embedding layer
  load_tokenizer = false
  num_workers = 0  # TODO: check for potential issues with iter-style dataset for num_workers > 0
  pin_memory = false

[data]
  data_dir = './data/datasets/autophe'
  data_subdir = 'time_categorized_atc'  # time_categorized, time_categorized_atc, time_not_categorized
  log_dir = './logs'
  max_seq_len = 512

[train]
  optimizer = 'hyper-1'
  scheduler = 'linear'  # 'noam', 'linear' (not used if optimizer == 'hyper')
  lr = 0.001
  hyper_lr = 0.0001
  betas = [0.9, 0.998]  # initial momentum (not used if optimizer = 'hyper')
  weight_decay = 0.01  # initial weight decay (not used if optimizer = 'hyper'; note: [adam, radam] and adamw have different ways of applying weight decay)
  early_stopping_patience = 0  # set to 0 to not use early stopping (train for maximum number of steps/epoch)
  accumulate_grad_batches = 1  # number of steps over which gradient is accumulated
  max_tokens_per_batch = 4_096  # 4_096  # 16_384  # 65_536
  n_epochs = 0  # total training epochs: if n_epochs set to 0, n_steps will be used
  n_steps = 500_000  # total training steps: only used if n_epochs is set to 0
  n_steps_check_val = 10_000  # how many training steps between each validation
  n_sched_steps = 100_000  # (not used if optimizer = 'hyper')
  n_sched_warmup_steps = 1_000  # (not used if optimizer = 'hyper')

[models]
  [models.word2vec]
    task = 'skipgram'  # todo: implement bow
    input_keys = ['pos_center', 'pos_context', 'neg_context']
    label_keys = []
    n_neg_samples = 15  # 0 for using softmax over all vocab tokens
    d_embed = 256
    [models.word2vec.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1

  [models.fasttext]
    task = 'skipgram'  # todo: implement bow
    input_keys = ['pos_center', 'pos_context', 'neg_context']
    label_keys = []
    n_neg_samples = 15  # 0 for using softmax over all vocab tokens
    d_embed = 256
    [models.fasttext.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1

  [models.glove]
    task = 'cooc'
    input_keys = ['left', 'right']
    label_keys = ['cooc']
    load_cooc_data = true
    d_embed = 16
    [models.glove.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
  
  [models.elmo]
    task = 'lm'
    input_keys = ['sample']
    label_keys = ['sample']
    d_conv = [512, 512]
    k_size = [2, 3]  # should be ngram_len lists?
    d_embed_char = 256
    d_embed_word = 256
    d_lstm = 512
    n_lstm_layers = 2
    token_type = 'char'  # 'char' (classic elmo), 'word', 'both'
    dropout = 0.5
    [models.elmo.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
  
  [models.transformer]
    task = 'reagent_pred_mt'  # could add 'mt' here
    input_keys = ['src', 'tgt']  # 'tgt' present for teacher forcing
    label_keys = ['tgt']
    share_embeddings = true
    n_enc_layers = 4
    n_dec_layers = 4
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.transformer.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4

  [models.bert]
    task = 'mlm'  # 'mlm', 'reagent_pred_mlm'
    input_keys = ['masked']
    label_keys = ['masked_label', 'masked_label_ids']
    n_layers = 4
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.bert.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
      '[MASK]' = 5

  [models.bert_classifier]
    # - If load_pretrained_bert is true, bert_ckpt_path is used for bert params.
    # - If bert_ckpt_path is 'none', bert_classifier parameters will be used to
    #   identify the correct checkpoint for bert (may fail).
    # - Note: for fine-tuning, you might consider decreasing the learning rate.
    task = 'reagent_pred_cls'  # could add 'cls' here
    tokenizer_task = 'reagent_pred_mlm'   # to have the same vocabulary as bert
    input_keys = ['sample']
    label_keys = ['label']
    n_classes = 100  # 0: all reagents, > 0: most popular reagents
    load_pretrained_bert = true
    bert_path = './logs/reagent_pred/bert_ngram-word/version_0/'
    bert_grad_type = 'all'  # 'last', 'none', 'norm', 'all' (weights tuned in bert)

  [models.fnet]
    task = 'mlm'  # 'mlm', 'reagent_pred_mlm'
    input_keys = ['masked']
    label_keys = ['masked_label', 'masked_label_ids']
    n_layers = 4
    d_embed = 256
    d_ff = 2048
    n_heads = 8
    dropout = 0.1
    [models.fnet.special_tokens]
      '[PAD]' = 0
      '[UNK]' = 1
      '[CLS]' = 2
      '[SEP]' = 3
      '[END]' = 4
      '[MASK]' = 5
